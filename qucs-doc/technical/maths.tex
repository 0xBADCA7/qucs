%
% This document contains the chapter some mathematical background.
%
% Copyright (C) 2003, 2004, 2005, 2006 Stefan Jahn <stefan@lkcc.org>
%
% Permission is granted to copy, distribute and/or modify this document
% under the terms of the GNU Free Documentation License, Version 1.1
% or any later version published by the Free Software Foundation.
%

\chapter{Mathematical background}
%\addcontentsline{toc}{chapter}{Mathematical background}

\section{Solving linear equation systems}
%\addcontentsline{toc}{section}{Solving linear equation systems}
\label{sec:linEQS}

When dealing with non-linear networks the number of equation systems
to be solved depends on the required precision of the solution and the
average necessary iterations until the solution is stable.  This
emphasizes the meaning of the solving procedures choice for different
problems.

\addvspace{12pt}

The equation systems
\begin{equation}
\left[A\right] \cdot \left[x\right] = \left[z\right]
\end{equation}
solution can be written as
\begin{equation}
\left[x\right] = \left[A\right]^{-1} \cdot \left[z\right]
\end{equation}

\subsection{Matrix inversion}
%\addcontentsline{toc}{subsection}{Matrix inversion}

The elements $\beta_{\mu\nu}$ of the inverse of the matrix $A$ are
\begin{equation}
\beta_{\mu\nu} = \frac{A_{\mu\nu}}{det A}
\end{equation}
whereas $A_{\mu\nu}$ is the matrix elements $a_{\mu\nu}$ cofactor.
The cofactor is the sub determinant (i.e. the minor) of the element
$a_{\mu\nu}$ multiplied with $(-1)^{\mu + \nu}$.  The determinant of a
square matrix can be recursively computed by either of the following
equations.
\begin{align}
det A = \sum_{\mu = 1}^{n} a_{\mu\nu}\cdot A_{\mu\nu}
\quad &\text{using the $\nu$-th column}\\
det A = \sum_{\nu = 1}^{n} a_{\mu\nu}\cdot A_{\mu\nu}
\quad &\text{using the $\mu$-th row}
\end{align}

This method is called the Laplace expansion.  In order to save
computing time the row or column with the most zeros in it is used for
the expansion expressed in the above equations.  A sub determinant
$(n-1)$-th order of a matrix's element $a_{\mu\nu}$ of $n$-th order is
the determinant which is computed by canceling the $\mu$-th row and
$\nu$-th column.  The following example demonstrates calculating the
determinant of a 4th order matrix with the elements of the 3rd row.
\begin{align}
\begin{vmatrix}
a_{11} & a_{12} & a_{13} & a_{14}\\
a_{21} & a_{22} & a_{23} & a_{24}\\
a_{31} & a_{32} & a_{33} & a_{34}\\
a_{41} & a_{42} & a_{43} & a_{44}\\
\end{vmatrix}
&= a_{31}
\begin{vmatrix}
a_{12} & a_{13} & a_{14}\\
a_{22} & a_{23} & a_{24}\\
a_{42} & a_{43} & a_{44}\\
\end{vmatrix}
- a_{32}
\begin{vmatrix}
a_{11} & a_{13} & a_{14}\\
a_{21} & a_{23} & a_{24}\\
a_{41} & a_{43} & a_{44}\\
\end{vmatrix}\\
\nonumber
&+ a_{33}
\begin{vmatrix}
a_{11} & a_{12} & a_{14}\\
a_{21} & a_{22} & a_{24}\\
a_{41} & a_{42} & a_{44}\\
\end{vmatrix}
- a_{34}
\begin{vmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{41} & a_{42} & a_{43}\\
\end{vmatrix}
\end{align}

This recursive process for computing the inverse of a matrix is most
easiest to be implemented but as well the slowest algorithm.  It
requires approximately $n!$ operations.

\subsection{Gaussian elimination}
%\addcontentsline{toc}{subsection}{Gaussian elimination}

The Gaussian algorithm for solving a linear equation system is done in
two parts: forward elimination and backward substitution.  During
forward elimination the matrix A is transformed into an upper
triangular equivalent matrix.  Elementary transformations due to an
equation system having the same solutions for the unknowns as the
original system.

\begin{equation}
A =
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
a_{21} & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
a_{n1} & a_{n2} & \ldots & a_{nn}
\end{bmatrix}
\rightarrow
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
0 & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & \ldots & 0 & a_{nn}
\end{bmatrix}
\end{equation}

The modifications applied to the matrix A in order to achieve this
transformations are limited to the following set of operations.
\begin{itemize}
\item multiplication of a row with a scalar factor
\item addition or subtraction of multiples of rows
\item exchanging two rows of a matrix
\end{itemize}

\subsubsection{Step 1: Forward elimination}
%\addcontentsline{toc}{subsubsection}{Step 1: Forward elimination}

The transformation of the matrix A is done in $\mathrm{n - 1}$
elimination steps.  The new matrix elements of the k-th step with
$\mathrm{k = 1, \ldots, n - 1}$ are computed with the following
recursive formulas.

\begin{align}
a_{ij} &= 0 & i = k+1, \ldots, n &\text{ and } j = k\\
a_{ij} &= a_{ij} - a_{kj} \cdot a_{ik} / a_{kk} & i = k+1, \ldots, n &\text{ and } j = k+1, \ldots, n\\
z_{i} &= z_{i} - z_{k} \cdot a_{ik} / a_{kk} & i = k+1, \ldots, n &
\end{align}

The triangulated matrix can be used to calculate the determinant very
easily.  The determinant of a triangulated matrix is the product of
the diagonal elements.  If the determinant $det A$ is non-zero the
equation system has a solution.  Otherwise the matrix A is singular.

\begin{equation}
det A = a_{11}\cdot a_{22}\cdot \ldots \cdot a_{nn} = \prod_{i=1}^{n} a_{ii}
\end{equation}

When using row and/or column pivoting the resulting determinant may
differ in its sign and must be multiplied with $(-1)^m$ whereas $m$ is
the number of row and column substitutions.

\subsubsection{Finding an appropriate pivot element}
%\addcontentsline{toc}{subsubsection}{Finding an appropriate pivot element}

The Gaussian elimination fails if the pivot element $a_{kk}$ turns to
be zero (division by zero).  That is why row and/or column pivoting
must be used before each elimination step.  If a diagonal element
$a_{kk} = 0$, then exchange the pivot row $k$ with the row $m > k$
having the coefficient with the largest absolute value.  The new pivot
row is $m$ and the new pivot element is going to be $a_{mk}$.  If no
such pivot row can be found the matrix is singular.

\addvspace{12pt}

Total pivoting looks for the element with the largest absolute value
within the matrix and exchanges rows and columns.  When exchanging
columns in equation systems the unknowns get reordered as well.  For
the numerical solution of equation systems with Gaussian elimination
column pivoting is clever, and total pivoting recommended.

\addvspace{12pt}

In order to improve numerical stability pivoting should also be
applied if $a_{kk} \ne 0$ because division by small diagonal elements
propagates numerical (rounding) errors.  This appears especially with
poorly conditioned (the two dimensional case: two lines with nearly
the same slope) equation systems.

\subsubsection{Step 2: Backward substitution}
%\addcontentsline{toc}{subsubsection}{Step 2: Backward substitution}

The solutions in the vector x are obtained by backward substituting
into the triangulated matrix.  The elements of the solution vector x
are computed by the following recursive equations.

\begin{align}
x_{n} &= \frac{z_{n}}{a_{nn}}\\
x_{i} &= \frac{z_{i}}{a_{ii}} - \sum_{k=i+1}^{n} x_{k}\cdot \frac{a_{ik}}{a_{ii}} & i = n - 1,\ldots,1
\end{align}

The forward elimination in the Gaussian algorithm requires
approximately $n^3/3$, the backward substitution $n^2/2$ operations.

\subsection{Gauss-Jordan method}
%\addcontentsline{toc}{subsection}{Gauss-Jordan method}

The Gauss-Jordan method is a modification of the Gaussian elimination.
In each k-th elimination step the elements of the k-th column get zero
except the diagonal element which gets 1.  When the right hand side
vector z is included in each step it contains the solution vector x
afterwards.

\addvspace{12pt}

The following recursive formulas must be applied to get the new matrix
elements for the k-th elimination step.  The k-th row must be computed
first.
\begin{align}
a_{kj} &= a_{kj} / a_{kk} & j = 1\ldots n\\
z_{k}  &= z_{k} / a_{kk}  &
\end{align}

Then the other rows can be calculated with the following formulas.
\begin{align}
a_{ij} &= a_{ij} - a_{ik}\cdot a_{kj} & j = 1,\ldots,n \textrm{ and } i = 1,\ldots,n \textrm{ with } i \ne k\\
z_{i}  &= z_{i} - a_{ik}\cdot z_{k}   & i = 1,\ldots,n \textrm{ with } i \ne k
\end{align}

Column pivoting may be necessary in order to avoid division by zero.
The solution vector x is not harmed by row substitutions.  When the
Gauss-Jordan algorithm has been finished the original matrix has been
transformed into the identity matrix.  If each operation during this
process is applied to an identity matrix the resulting matrix is the
inverse matrix of the original matrix.  This means that the
Gauss-Jordan method can be used to compute the inverse of a matrix.

\addvspace{12pt}

Though this elimination method is easy to implement the number of
required operations is larger than within the Gaussian elimination.
The Gauss-Jordan method requires approximately $N^3/2 + N^2/2$
operations.

\subsection{LU decomposition}
%\addcontentsline{toc}{subsection}{LU decomposition}

LU decomposition (decomposition into a lower and upper triangular
matrix) is recommended when dealing with equation systems where the
matrix A does not alter but the right hand side (the vector z) does.
Both the Gaussian elimination and the Gauss-Jordan method involve both
the right hand side and the matrix in their algorithm.  Consecutive
solutions of an equation system with an altering right hand side can
be computed faster with LU decomposition.

\addvspace{12pt}

The LU decomposition splits a matrix A into a product of a lower
triangular matrix L with an upper triangular matrix U.

\begin{equation}
A = L U \;\text{ with }\;
L = 
\begin{bmatrix}
l_{11} & 0 & \ldots & 0\\
l_{21} & l_{22} & \ddots & \vdots\\
\vdots &  & \ddots & 0\\
l_{n1} & \ldots & \ldots & l_{nn}
\end{bmatrix}
\;\text{ and }\;
U =
\begin{bmatrix}
u_{11} & u_{12} & \ldots & u_{1n}\\
0 & u_{22} &  & \vdots\\
\vdots & \ddots & \ddots & \vdots\\
0 & \ldots & 0 & u_{nn}
\end{bmatrix}
\end{equation}

The algorithm for solving the linear equation system $Ax = z$ involves
three steps:
\begin{itemize}
\item LU decomposition of the coefficient matrix A\\
$\rightarrow Ax = LUx = z$
\item introduction of an (unknown) arbitrary vector y and solving the equation system $Ly = z$ by forward substitution\\
$\rightarrow y = Ux = L^{-1}z$
\item solving the equation system $Ux = y$  by backward substitution\\
$\rightarrow x = U^{-1}y$
\end{itemize}

The decomposition of the matrix A into a lower and upper triangular
matrix is not unique.  The most important decompositions, based on
Gaussian elimination, are the Doolittle, the Crout and the Cholesky
decomposition.

\addvspace{12pt}

If pivoting is necessary during these algorithms they do not decompose
the matrix $A$ but the product with an arbitrary matrix $PA$ (a
permutation of the matrix $A$).  When exchanging rows and columns the
order of the unknowns as represented by the vector $z$ changes as well
and must be saved during this process for the forward substitution in
the algorithms second step.

\subsubsection{Step 1: LU decomposition}
%\addcontentsline{toc}{subsubsection}{Step 1: LU decomposition}

Using the decomposition according to Crout the coefficients of the L
and U matrices can be stored in place the original matrix A.  The
upper triangular matrix U has the form

\begin{equation}
U = 
\begin{bmatrix}
1 & u_{12} & \ldots & u_{1n}\\
0 & 1 &  & \vdots\\
\vdots & \ddots & \ddots & u_{n-1,n}\\
0 & \ldots & 0 & 1
\end{bmatrix}
\label{eq:CroutU}
\end{equation}

The diagonal elements $u_{jj}$ are ones and thus the determinant $det
U$ is one as well.  The elements of the new coefficient matrix $LU$
for the k-th elimination step with $k = 1, \ldots,n$ compute as
follows:
\begin{align}
u_{jk} &= \frac{1}{l_{jj}}\left(a_{jk} - \sum_{r=1}^{j-1} l_{jr} u_{rk}\right) & j &= 1,\ldots,k-1\\
l_{jk} &= a_{jk} - \sum_{r=1}^{k-1} l_{jr} u_{rk} & j &= k,\ldots,n
\end{align}

Pivoting may be necessary as you are going to divide by the diagonal
element $l_{jj}$.

\subsubsection{Step 2: Forward substitution}
%\addcontentsline{toc}{subsubsection}{Step 2: Forward substitution}
\label{sec:CroutFSubst}

The solutions in the arbitrary vector $y$ are obtained by forward
substituting into the triangulated $L$ matrix.  At this stage you need
to remember the order of unknowns in the vector $z$ as changed by
pivoting.  The elements of the solution vector $y$ are computed by the
following recursive equation.

\begin{align}
y_{i} &= \frac{z_{i}}{l_{ii}} - \sum_{k=1}^{i-1} y_{k}\cdot \frac{l_{ik}}{l_{ii}} & i = 1,\ldots,n
\end{align}

\subsubsection{Step 3: Backward substitution}
%\addcontentsline{toc}{subsubsection}{Step 3: Backward substitution}
\label{sec:CroutBSubst}

The solutions in the vector $x$ are obtained by backward substituting
into the triangulated $U$ matrix.  The elements of the solution vector
$x$ are computed by the following recursive equation.

\begin{align}
x_{i} &= y_{i} - \sum_{k=i+1}^{n} x_{k}\cdot u_{ik} & i = n,\ldots,1
\end{align}

The division by the diagonal elements of the matrix U is not necessary
because of Crouts definition in eq. (\ref{eq:CroutU}) with $u_{ii} =
1$.

\addvspace{12pt}

The LU decomposition requires approximately $n^3/3 + n^2 - n/3$
operations for solving a linear equation system.  For $M$ consecutive
solutions the method requires $n^3/3 + Mn^2 - n/3$ operations.

\subsection{QR decomposition}
%\addcontentsline{toc}{subsection}{QR decomposition}

Singular matrices actually having a solution are over- or
under-determined.  These types of matrices can be handled by three
different types of decompositions: Householder, Jacobi (Givens
rotation) and singular value decomposition.  Householder decomposition
factors a matrix $A$ into the product of an orthonormal matrix $Q$ and
an upper triangular matrix $R$, such that:
\begin{equation}
A = Q\cdot R
\end{equation}

The Householder decomposition is based on the fact that for any two
different vectors, $v$ and $w$, with $\left\lVert v\right\rVert =
\left\lVert w\right\rVert$, i.e. different vectors of equal length, a
reflection matrix $H$ exists such that:
\begin{equation}
H \cdot v = w
\end{equation}

To obtain the matrix $H$, the vector $u$ is defined by:
\begin{equation}
u = \dfrac{v - w}{\left\lVert v - w\right\rVert}
\end{equation}

The matrix $H$ defined by 
\begin{equation}
\label{eq:ReflectionMatrix}
H = I - 2 \cdot u \cdot u^T
\end{equation}

is then the required reflection matrix.

\addvspace{12pt}

The equation system
\begin{equation}
A\cdot x = z
\;\;\;\; \textrm{ is transformed into } \;\;\;\;
Q R\cdot x = z
\end{equation}

With $Q^T\cdot Q = I$ this yields
\begin{equation}
Q^T Q R\cdot x = Q^T z
\;\;\;\; \rightarrow \;\;\;\;
R\cdot x = Q^T z
\end{equation}

Since $R$ is triangular the equation system is solved by a simple
matrix-vector multiplication on the right hand side and backward
substitution.

\subsubsection{Step 1: QR decomposition}
%\addcontentsline{toc}{subsubsection}{Step 1: QR decomposition}

Starting with $A_1 = A$, let $v_1$ = the first column of $A_1$, and
$w_1^T = \left(\pm\lVert v_1\rVert , 0 , \ldots 0\right)$, i.e. a column
vector whose first component is the norm of $v_1$ with the remaining
components equal to 0.  The Householder transformation $H_1 = I - 2
\cdot u_1 \cdot u_1^T$ with $u_1 = v_1 - w_1 / \lVert v_1 - w_1
\rVert$ will turn the first column of $A_1$ into $w_1$ as with $H_1
\cdot A_1 = A_2$.  At each stage $k$, $v_k$ = the kth column of $A_k$
on and below the diagonal with all other components equal to 0, and
$w_k$'s kth component equals the norm of $v_k$ with all other
components equal to 0.  Letting $H_k \cdot A_k = A_{k+1}$, the
components of the kth column of $A_{k+1}$ below the diagonal are each
0.  These calculations are listed below for each stage for the matrix
A.

\begin{equation}
\begin{split}
v_1 =
\begin{bmatrix}
a_{11}\\
a_{21}\\
\vdots\\
a_{n1}
\end{bmatrix}
\;\;\;
w_1 =
\begin{bmatrix}
\pm\lVert v_1 \rVert\\
0\\
\vdots\\
0
\end{bmatrix}
\;\;\;
u_1 = \dfrac{v_1 - w_1}{\left\lVert v_1 - w_1\right\rVert} =
\begin{bmatrix}
u_{11}\\
u_{21}\\
\vdots\\
u_{n1}
\end{bmatrix}
\\
H_1 = I - 2 \cdot u_1 \cdot u_1^T
\;\;\; \rightarrow \;\;\;
H_1 \cdot A_1 = A_2 =
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
0 & a_{22} & \ldots & a_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & a_{n2} & \ldots & a_{nn}
\end{bmatrix}
\end{split}
\end{equation}

With this first step the upper left diagonal element of the $R$
matrix, $a_{11} = \pm\lVert v_1 \rVert$, has been generated.  The
elements below are zeroed out.  Since $H_1$ can be generated from
$u_1$ stored in place of the first column of $A_1$ the multiplication
$H_1 \cdot A_1$ can be performed without actually generating $H_1$.

\begin{equation}
\begin{split}
v_2 =
\begin{bmatrix}
0\\
a_{22}\\
\vdots\\
a_{n2}
\end{bmatrix}
\;\;\;
w_1 =
\begin{bmatrix}
0\\
\pm\lVert v_2 \rVert\\
\vdots\\
0
\end{bmatrix}
\;\;\;
u_2 = \dfrac{v_2 - w_2}{\left\lVert v_2 - w_2\right\rVert} =
\begin{bmatrix}
0\\
u_{22}\\
\vdots\\
u_{n2}
\end{bmatrix}
\\
H_2 = I - 2 \cdot u_2 \cdot u_2^T
\;\;\; \rightarrow \;\;\;
H_2 \cdot A_2 = A_3 =
\begin{bmatrix}
a_{11} & a_{12} & \ldots & a_{1n}\\
0 & a_{22} & \ldots & a_{2n}\\
\vdots & 0 & \ddots & \vdots\\
0 & 0 &  & a_{nn}
\end{bmatrix}
\end{split}
\end{equation}

These elimination steps generate the $R$ matrix because $Q$ is
orthonormal, i.e.
\begin{equation}
\begin{split}
A = Q\cdot R
\;\;\; \rightarrow \;\;\;
Q^T A = Q^T Q\cdot R
\;\;\; \rightarrow \;\;\;
Q^T A = R
\\
\;\;\; \rightarrow \;\;\;
H_n \cdot \ldots \cdot H_2 \cdot H_1 \cdot A = R
\end{split}
\end{equation}

\addvspace{12pt}

After $n$ elimination steps the original matrix $A$ contains the upper
triangular matrix $R$, except for the diagonal elements which can be
stored in some vector.  The lower triangular matrix contains the
Householder vectors $u_1 \ldots u_n$.
\begin{equation}
A =
\begin{bmatrix}
u_{11} & r_{12} & \ldots & r_{1n}\\
u_{21} & u_{22} &  & r_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
u_{n1} & u_{n2} & \ldots & u_{nn}
\end{bmatrix}
\;\;\;\;
R_{diag} = 
\begin{bmatrix}
r_{11}\\
r_{22}\\
\vdots\\
r_{nn}
\end{bmatrix}
\end{equation}

With $Q^T = H_1 \cdot H_2 \cdot \ldots \cdot H_n$ this representation
contains both the $Q$ and $R$ matrix, in a packed form, of course: $Q$
as a composition of Householder vectors and $R$ in the upper
triangular part and its diagonal vector $R_{diag}$.

\subsubsection{Step 2: Forming the new right hand side}
%\addcontentsline{toc}{subsubsection}{Step 2: Forming the new right hand side}

In order to form the right hand side $Q^T z$ let remember
eq. \eqref{eq:ReflectionMatrix} denoting the reflection matrices used
to compute $Q^T$.
\begin{equation}
H_n\cdot \ldots \cdot H_2\cdot H_1 = Q^T
\end{equation}

Thus it is possible to replace the original right hand side vector $z$
by
\begin{equation}
H_n\cdot \ldots \cdot H_2\cdot H_1\cdot z = Q^T \cdot z
\end{equation}

which yields for each $k = 1\ldots n$ the following expression:
\begin{equation}
\label{eq:QTz}
H_k \cdot z = \left(I - 2\cdot u_k \cdot u_k^T\right)\cdot z = z - 2\cdot u_k \cdot u_k^T\cdot z
\end{equation}

The latter $u_k^T\cdot z$ is a simple scalar product of two vectors.
Performing eq. \eqref{eq:QTz} for each Householder vector finally
results in the new right hand side vector $Q^T z$.

\subsubsection{Step 3: Backward substitution}
%\addcontentsline{toc}{subsubsection}{Step 3: Backward substitution}

The solutions in the vector $x$ are obtained by backward substituting
into the triangulated $R$ matrix.  The elements of the solution vector
$x$ are computed by the following recursive equation.

\begin{align}
x_{i} &= \dfrac{z_{i}}{r_{ii}} - \sum_{k=i+1}^{n} x_{k}\cdot \dfrac{r_{ik}}{r_{ii}} & i = n,\ldots,1
\end{align}

\subsubsection{Motivation}
%\addcontentsline{toc}{subsubsection}{Motivation}

Though the QR decomposition has an operation count of $2n^3 + 3n^2$
(which is about six times more than the LU decomposition) it has its
advantages.  The QR factorization method is said to be unconditional
stable and more accurate.  Also it can be used to obtain the
minimum-norm (or least square) solution of under-determined equation
systems.
\begin{figure}[ht]
\begin{center}
\includegraphics[width=10cm]{MNAsingular}
\end{center}
\caption{circuit with singular modified nodal analysis matrix}
\label{fig:MNAsingular}
\end{figure}
\FloatBarrier

The circuit in fig. \ref{fig:MNAsingular} has the following MNA
representation:
\begin{equation}
A x =
\begin{bmatrix}
\frac{1}{R_{2}} & 0 & 0\\
0 & \frac{1}{R_{1}} & -\frac{1}{R_{1}}\\
0 & -\frac{1}{R_{1}} & \frac{1}{R_{1}}
\end{bmatrix}
\cdot
\begin{bmatrix}
V_1\\
V_2\\
V_3
\end{bmatrix}
=
\begin{bmatrix}
0.1 & 0 & 0\\
0 & 0.1 & -0.1\\
0 & -0.1 & 0.1
\end{bmatrix}
\cdot
\begin{bmatrix}
V_1\\
V_2\\
V_3
\end{bmatrix}
=
\begin{bmatrix}
I_1\\
-I_1\\
I_2
\end{bmatrix}
=
\begin{bmatrix}
0.1\\
-0.1\\
0.1
\end{bmatrix}
=
z
\end{equation}

The second and third row of the matrix $A$ are linear dependent and
the matrix is singular because its determinant is zero.  Depending on
the right hand side $z$, the equation system has none or unlimited
solutions.  This is called an under-determined system.  The discussed
QR decomposition easily computes a valid solution without reducing
accuracy.  The LU decomposition would probably fail because of the
singularity.

\subsubsection{QR decomposition with column pivoting}
%\addcontentsline{toc}{subsubsection}{QR decomposition with column pivoting}

\subsubsection{Least norm problem}
%\addcontentsline{toc}{subsubsection}{Least norm problem}

With some more effort it is possible to obtain the minimum-norm
solution of this problem.  The algorithm as described here would
probably yield the following solution:
\begin{equation}
x =
\begin{bmatrix}
V_1\\
V_2\\
V_3
\end{bmatrix}
=
\begin{bmatrix}
1\\
0\\
1
\end{bmatrix}
\end{equation}

This is one out of unlimited solutions.  The following short
description shows how it is possible to obtain the minimum-norm
solution.  When decomposing the transposed problem
\begin{equation}
A^T = Q\cdot R
\end{equation}

the minimum-norm solution $\hat{x}$ is obtained by forward substitution of
\begin{equation}
R^T\cdot x = z
\end{equation}

and multiplying the result with $Q$.
\begin{equation}
\hat{x} = Q\cdot x
\end{equation}

In the example above this algorithm results in a solution vector with
the least vector norm possible:
\begin{equation}
\hat{x} =
\begin{bmatrix}
V_1\\
V_2\\
V_3
\end{bmatrix}
=
\begin{bmatrix}
1\\
-0.5\\
0.5
\end{bmatrix}
\end{equation}

This algorithm outline is also sometimes called LQ decomposition
because of $R^T$ being a lower triangular matrix used by the forward
substitution.

\subsection{Singular value decomposition}
%\addcontentsline{toc}{subsection}{Singular value decomposition}

Very bad conditioned (ratio between largest and smallest eigenvalue)
matrices, i.e. nearly singular, or even singular matrices (over- or
under-determined equation systems) can be handled by the singular
value decomposition (SVD).  This type of decomposition is defined by
\begin{equation}
\label{eq:USV}
A = U\cdot \Sigma\cdot V^H
\end{equation}

where the $U$ matrix consists of the orthonormalized eigenvectors
associated with the eigenvalues of $A\cdot A^H$, $V$ consists of the
orthonormalized eigenvectors of $A^H\cdot A$ and $\Sigma$ is a matrix
with the singular values of $A$ (non-negative square roots of the
eigenvalues of $A^H\cdot A$) on its diagonal and zeros otherwise.
\begin{equation}
\Sigma =
\begin{bmatrix}
\sigma_1 & 0 & \cdots & 0\\
0 & \sigma_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \sigma_n
\end{bmatrix}
\end{equation}

The singular value decomposition can be used to solve linear equation
systems by simple substitutions
\begin{align}
A\cdot x &= z\\
U\cdot \Sigma\cdot V^H\cdot x &= z\\
\Sigma\cdot V^H\cdot x &= U^H\cdot z
\end{align}
since
\begin{equation}
U^H\cdot U = V^H\cdot V = V\cdot V^H = I
\end{equation}

To obtain the decomposition stated in eq.~\eqref{eq:USV} Householder
vectors are computed and their transformations are applied from the
left-hand side and right-hand side to obtain an upper bidiagonal
matrix $B$ which has the same singular values as the original $A$
matrix because all of the transformations introduced are orthogonal.
\begin{equation}
U_B^{H\,(n)}\cdot\ldots\cdot U_B^{H\,(1)}\cdot A\cdot
V_B^{(1)}\cdot\ldots\cdot V_B^{(n-2)}
= U_B^H\cdot A\cdot V_B = B^{(0)}
\end{equation}

Specifically, $U_B^{H\,(i)}$ annihilates the subdiagonal elements in
column $i$ and $V_B^{(j)}$ zeros out the appropriate elements in row
$j$.
\begin{equation}
B^{(0)} = 
\begin{bmatrix}
\beta_1 & \delta_2 & 0 & \cdots & 0\\
0 & \beta_2 & \delta_3 & 0 & 0\\
\vdots & 0 & \ddots & \ddots & 0\\
0 & 0 & 0 & \beta_{n-1} & \delta_n\\
0 & 0 & 0 & 0 & \beta_n\\
\end{bmatrix}
\end{equation}

Afterwards an iterative process (which turns out to be a QR iteration)
is used to transform the bidiagonal matrix $B$ into a diagonal form by
applying successive Givens transformations (therefore orthogonal as
well) to the bidiagonal matrix.  This iteration is said to have cubic
convergence and yields the final singular values of the matrix $A$.
\begin{equation}
B^{(0)} \rightarrow B^{(1)}\rightarrow \ldots \rightarrow \Sigma
\end{equation}
\begin{equation}
B^{(k+1)} = \left(S^{(k)}\right)^H\cdot B^{(k)}\cdot T^{(k)}
\end{equation}

Each of the transformations applied to the bidiagonal matrix is also
applied to the matrices $U_B$ and $V_B^H$ which finally yield the $U$
and $V^H$ matrices after convergence.

\addvspace{12pt}

So far for the algorithm outline.  Without the very details the
following sections briefly describe each part of the singular value
decomposition.

\subsubsection{Notation}

Beforehand some notation marks are going to be defined.

\begin{itemize}

\item Conjugate transposed (or adjoint):
\begin{equation*}
A \rightarrow \left(A^T\right)^* = \left(A^*\right)^T = A^H
\end{equation*}


\item Euclidian norm:
\begin{equation*}
\lVert x \rVert = \sqrt{\sum^n_{i=1} x_i\cdot x^*_1} =
\sqrt{\sum^n_{i=1} \lvert x_i\rvert^2} = 
\sqrt{\lvert x_1\rvert^2 + \cdots  + \lvert x_n\rvert^2} = 
\sqrt{x\cdot x^H}
\end{equation*}

\item Hermitian (or self adjoint):
\begin{equation*}
A = A^H
\end{equation*}
whereas $A^H$ denotes the conjugate transposed matrix of $A$.  In the
real case the matrix $A$ is then said to be ``symmetric''.

\item Unitary:
\begin{equation*}
A\cdot A^H = A^H\cdot A = I
\end{equation*}
Real matrices $A$ with this property are called ``orthogonal''.

\end{itemize}

\subsubsection{Householder reflector}

A Householder matrix is an elementary unitary matrix that is
Hermitian.  Its fundamental use is their ability to transform a vector
$x$ to a multiple of $\vec{e}_1$, the first column of the identity
matrix.  The elementary Hermitian (i.e. the Householder matrix) is
defined as
\begin{equation}
H = I - 2\cdot u\cdot u^H
\;\;\;\;
\textrm{ where }
\;\;\;\;
u^H\cdot u = 1
\end{equation}

Beside excellent numerical properties, their application demonstrates
their efficiency.  If $A$ is a matrix, then
\begin{align}
H\cdot A &= A - 2\cdot u\cdot u^H\cdot A\\
\nonumber
&=  A - 2\cdot u\cdot \left(A^H\cdot u\right)^H
\end{align}
and hence explicit formation and storage of $H$ is not required.  Also
colums (or rows) can be transformed individually exploiting the fact
that $u^H\cdot A$ yields a scalar product for single columns or rows.

\paragraph{Specific case}

In order to reduce a 4$\times$4 matrix $A$ to upper triangular form
successive Householder reflectors must be applied.
\begin{equation}
A =
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & a_{14}\\
a_{21} & a_{22} & a_{23} & a_{24}\\
a_{31} & a_{32} & a_{33} & a_{34}\\
a_{41} & a_{42} & a_{43} & a_{44}
\end{bmatrix}
\end{equation}

In the first step the diagonal element $a_{11}$ gets replaced and its
below elements get annihilated by the multiplication with an
appropriate Householder vector, also the remaining right-hand columns
get modified.
\begin{equation}
u_1 =
\begin{bmatrix}
u_{11}\\
u_{21}\\
u_{31}\\
u_{41}
\end{bmatrix}
\;\;\;\;
H_1 = I - 2\cdot u_1\cdot u_1^H
\;\;\;\;
\rightarrow A_1 = H_1\cdot A =
\begin{bmatrix}
\beta_1 & a_{12}^{(1)} & a_{13}^{(1)} & a_{14}^{(1)}\\
0 & a_{22}^{(1)} & a_{23}^{(1)} & a_{24}^{(1)}\\
0 & a_{32}^{(1)} & a_{33}^{(1)} & a_{34}^{(1)}\\
0 & a_{42}^{(1)} & a_{43}^{(1)} & a_{44}^{(1)}
\end{bmatrix}
\end{equation}

This process must be repeated
\begin{equation}
u_2 =
\begin{bmatrix}
0\\
u_{22}\\
u_{32}\\
u_{42}
\end{bmatrix}
\;\;\;\;
H_2 = I - 2\cdot u_2\cdot u_2^H
\;\;\;\;
\rightarrow A_2 = H_2\cdot A_1 =
\begin{bmatrix}
\beta_1 & a_{12}^{(2)} & a_{13}^{(2)} & a_{14}^{(2)}\\
0 & \beta_2 & a_{23}^{(2)} & a_{24}^{(2)}\\
0 & 0 & a_{33}^{(2)} & a_{34}^{(2)}\\
0 & 0 & a_{43}^{(2)} & a_{44}^{(2)}
\end{bmatrix}
\end{equation}

\begin{equation}
u_3 =
\begin{bmatrix}
0\\
0\\
u_{33}\\
u_{43}
\end{bmatrix}
\;\;\;\;
H_3 = I - 2\cdot u_3\cdot u_3^H
\;\;\;\;
\rightarrow A_3 = H_3\cdot A_2 =
\begin{bmatrix}
\beta_1 & a_{12}^{(3)} & a_{13}^{(3)} & a_{14}^{(3)}\\
0 & \beta_2 & a_{23}^{(3)} & a_{24}^{(3)}\\
0 & 0 & \beta_3 & a_{34}^{(3)}\\
0 & 0 & 0 & a_{44}^{(3)}
\end{bmatrix}
\end{equation}

\begin{equation}
u_4 =
\begin{bmatrix}
0\\
0\\
0\\
u_{44}
\end{bmatrix}
\;\;\;\;
H_4 = I - 2\cdot u_4\cdot u_4^H
\;\;\;\;
\rightarrow A_4 = H_4\cdot A_3 =
\begin{bmatrix}
\beta_1 & a_{12}^{(4)} & a_{13}^{(4)} & a_{14}^{(4)}\\
0 & \beta_2 & a_{23}^{(4)} & a_{24}^{(4)}\\
0 & 0 & \beta_3 & a_{34}^{(4)}\\
0 & 0 & 0 & \beta_4
\end{bmatrix}
\end{equation}

until the matrix $A$ contains an upper triangular matrix $R$.  The
matrix $Q$ can be expressed as the the product of the Householder
vectors.  The performed operations deliver
\begin{equation}
H^H_4\cdot H^H_3\cdot H^H_2\cdot H^H_1 \cdot A = Q^H\cdot A = R
\;\;\;\; \rightarrow \;\;\;\;
A = Q\cdot R
\end{equation}

since $Q$ is unitary.  The matrix $Q$ itself can be expressed in terms
of $H_i$ using the following transformation.
\begin{align}
\label{eq:qh_1}
Q^H &= H^H_4\cdot H^H_3\cdot H^H_2\cdot H^H_1\\
\left(Q^H\right)^H &= \left(H^H_4\cdot H^H_3\cdot H^H_2\cdot H^H_1\right)^H\\
\label{eq:qh_3}
Q &= H_1\cdot H_2\cdot H_3\cdot H_4
\end{align}

The eqn.~\eqref{eq:qh_1}-\eqref{eq:qh_3} are necessary to be mentioned
only in case $Q$ is not Hermitian, but still unitary.  Otherwise there
is no difference computing $Q$ or $Q^H$ using the Householder vectors.
No care must be taken in choosing forward or backward accumulation.

\paragraph{General case}

In the general case it is necessary to find an elementary unitary
matrix
\begin{equation}
H = I - \tau\cdot u\cdot u^H
\end{equation}
that satisfies the following three conditions.
\begin{equation}
\label{eq:herm_cond}
\left|\tau\right|^2\cdot u^H\cdot u = \tau + \tau^* =
2\cdot Re\left\{\tau \right\}
\;\;\;\; , \;\;\;\;
H^H\cdot x = \gamma\cdot \lVert x \rVert \cdot \vec{e}_1
\;\;\;\; , \;\;\;\;
\lvert \gamma \rvert = 1
\end{equation}

When choosing the elements $u_{ii} = 1$ it is possible the store the
Householder vectors as well as the upper triangular matrix $R$ in the
same storage of the matrix $A$.  The Householder matrices $H_i$ can be
completely restored from the Householder vectors.
\begin{equation}
A =
\begin{bmatrix}
\beta_1 & a_{12} & a_{13} & a_{14}\\
u_{21} & \beta_2 & a_{23} & a_{24}\\
u_{31} & u_{32} & \beta_3 & a_{34}\\
u_{41} & u_{42} & u_{43} & \beta_4
\end{bmatrix}
\end{equation}

There exist several approaches to meet the conditions expressed in
eq. \eqref{eq:herm_cond}.  For fewer computational effort it may be
convenient to choose $\gamma$ to be real valued.  With the notation
\begin{equation}
H^H\cdot x =
H^H\cdot
\begin{bmatrix}
\alpha\\
x_2\\
x_3\\
x_4\\
\end{bmatrix}
=
\begin{bmatrix}
\beta\\
0\\
0\\
0\\
\end{bmatrix}
\end{equation}

one possibility is to define the following calculation rules.
\begin{align}
\nu &= sign\left(Re\left\{\alpha\right\}\right)\cdot \lVert x \rVert\\
\tau &= \dfrac{\alpha + \nu}{\nu}\\
\gamma &= -1\\
\beta &= \gamma \cdot \lVert x \rVert = - \lVert x \rVert
\;\;\;\; \rightarrow \;\;\;\;
\textrm{real valued}\\
u &= \dfrac{x + \nu\cdot \vec{e}_1}{\alpha + \nu}
\;\;\;\; \rightarrow \;\;\;\;
u_{ii} = 1
\end{align}

These definitions yield a complex $\tau$, thus $H$ is no more
Hermitian but still unitary.
\begin{equation}
H = I - \tau\cdot u\cdot u^H
\;\;\;\; \rightarrow \;\;\;\;
H^H = I - \tau^*\cdot u\cdot u^H
\end{equation}

\subsubsection{Givens rotation}

A Givens rotation is a plane rotation matrix.  Such a plane rotation
matrix is an orthogonal matrix that is different from the identity
matrix only in four elements.
\begin{equation}
M =
\left[
\begin{array}{ccccccccccc}
1 & 0 & \cdots &  &  &  &  &  &  & \cdots & 0\\
0 & \ddots &  &  &  &  &  &  &  &  & \vdots\\
\vdots &  & 1 &  &  &  &  &  &  &  & \\
 &  &  & +c & 0 & \cdots & 0 & +s &  &  & \\
 &  &  & 0 & 1 &  &  & 0 &  &  & \\
 &  &  & \vdots &  & \ddots &  & \vdots &  &  & \\
 &  &  & 0 &  &  & 1 & 0 &  &  & \\
 &  &  & -s & 0 & \cdots & 0 & +c &  &  & \\
 &  &  &  &  &  &  &  & 1 &  & \vdots\\
\vdots &  &  &  &  &  &  &  &  & \ddots & 0\\
0 & \cdots &  &  &  &  &  &  & \cdots & 0 & 1
\end{array}
\right]
\end{equation}

The elements are usually choosen so that
\begin{equation}
\label{eq:givenscond}
R =
\left[\begin{array}{rl}
c & s\\
-s & c\\
\end{array}\right]
\;\;\;\;  \;\;\;\;
c = \cos{\theta},\; s = \sin{\theta}
\;\;\;\; \rightarrow \;\;\;\;
\left|c\right|^2 + \left|s\right|^2 = 1
\end{equation}

The most common use of such a plane rotation is to choose $c$ and $s$
such that for a given $a$ and $b$
\begin{equation}
\label{eq:givensrot}
R =
\left[\begin{array}{rl}
c & s\\
-s & c\\
\end{array}\right]
\cdot
\begin{bmatrix}
a\\
b\\
\end{bmatrix}
=
\begin{bmatrix}
d\\
0\\
\end{bmatrix}
\end{equation}
multiplication annihilates the lower vector entry.  In such an
application the matrix $R$ is often termed ``Givens rotation'' matrix.
The following equations satisfy eq.~\eqref{eq:givensrot} for a given
$a$ and $b$ exploiting the conditions given in
eq.~\eqref{eq:givenscond}.
\begin{equation}
c = \dfrac{\pm a}{\sqrt{\left|a\right|^2 + \left|b\right|^2}}
\;\;\;\; \textrm{and} \;\;\;\;
s = \dfrac{\pm b}{\sqrt{\left|a\right|^2 + \left|b\right|^2}}
\end{equation}
\begin{equation}
d = \sqrt{\left|a\right|^2 + \left|b\right|^2}
\end{equation}

\subsubsection{Step 1: Bidiagonalization}

In the first step the original matrix $A$ is bidiagonalized by the
application of Householder reflections from the left and right hand
side.  The matrices $U_B^H$ and $V_B$ can each be determined as a
product of Householder matrices.
\begin{equation}
\underbrace{U_B^{H\,(n)}\cdot\ldots\cdot U_B^{H\,(1)}}_{U_B^H}\cdot A\cdot
\underbrace{V_B^{(1)}\cdot\ldots\cdot V_B^{(n-2)}}_{V_B}
= U_B^H\cdot A\cdot V_B = B^{(0)}
\end{equation}

Each of the required Householder vectors are created and applied as
previously defined.  Suppose a $n\times n$ matrix, then applying the
first Householder vector from the left hand side eliminates the first
column and yields
\begin{align}
U_B^{H\,(1)} \cdot A &=
\begin{bmatrix}
\boldsymbol{\beta_1} & a_{12}^{(1)} & a_{13}^{(1)} & \cdots & a_{1n}^{(1)} \\
u_{21} & a_{22}^{(1)} & a_{23}^{(1)} &  & a_{2n}^{(1)} \\
u_{31} & a_{32}^{(1)} & a_{33}^{(1)} &  & a_{3n}^{(1)} \\
\vdots &  & & \ddots & \vdots \\
u_{n1} & a_{n2}^{(1)} & a_{n3}^{(1)} & \cdots & a_{nn}^{(1)} \\
\end{bmatrix}\\
%\end{align}
\intertext{
Next, a Householder vector is applied from the right hand side to
annihilate the first row.
}
%\begin{align}
U_B^{H\,(1)} \cdot A \cdot V_B^{(1)} &=
\begin{bmatrix}
\beta_1 & \boldsymbol{\delta_2} & v_{13} & \cdots & v_{1n} \\
u_{21} & a_{22}^{(2)} & a_{23}^{(2)} &  & a_{2n}^{(2)} \\
u_{31} & a_{32}^{(2)} & a_{33}^{(2)} &  & a_{3n}^{(2)} \\
\vdots &  & & \ddots & \vdots \\
u_{n1} & a_{n2}^{(2)} & a_{n3}^{(2)} & \cdots & a_{nn}^{(2)} \\
\end{bmatrix}\\
%\end{align}
\intertext{
Again, a Householder vector is applied from the left hand side to
annihilate the second column.
}
%\begin{align}
U_B^{H\,(2)} \cdot U_B^{H\,(1)} \cdot A \cdot V_B^{(1)} &=
\begin{bmatrix}
\beta_1 & \delta_2 & v_{13} & \cdots & v_{1n} \\
u_{21} & \boldsymbol{\beta_2} & a_{23}^{(3)} &  & a_{2n}^{(3)} \\
u_{31} & u_{32} & a_{33}^{(3)} &  & a_{3n}^{(3)} \\
\vdots & \vdots & & \ddots & \vdots \\
u_{n1} & u_{n2} & a_{n3}^{(3)} & \cdots & a_{nn}^{(3)} \\
\end{bmatrix}\\
%\end{align}
\intertext{
This process is continued until
}
%\begin{align}
U_B^H \cdot A \cdot V_B &=
\begin{bmatrix}
\beta_1 & \delta_2 & v_{13} & \cdots & v_{1n} \\
u_{21} & \beta_2 & \delta_3 &  & v_{2n} \\
u_{31} & u_{32} & \ddots & \ddots &  \\
\vdots & \vdots & & \beta_{n-1} & \delta_n \\
u_{n1} & u_{n2} & u_{n3} &  & \boldsymbol{\beta_n} \\
\end{bmatrix}
\end{align}

For each of the Householder transformations from the left and right
hand side the appropiate $\tau$ values must be stored in separate
vectors.

\subsubsection{Step 2: Matrix reconstructions}

Using the Householder vectors stored in place of the original $A$
matrix and the appropiate $\tau$ value vectors it is now necessary to
unpack the $U_B$ and $V_B^H$ matrices.  The diagonal vector $\beta$
and the super-diagonal vector $\delta$ can be saved in separate
vectors previously.  Thus the $U_B$ matrix can be unpacked in place of
the $A$ matrix and the $V_B^H$ matrix is unpacked in a separate
matrix.

\addvspace{12pt}

There are two possible algorithms for computing the Householder
product matrices, i.e. forward accumulation and backward accumulation.
Both start with the identity matrix which is successively multiplied
by the Householder matrices either from the left or right.
\begin{align}
U_B^H &= H^H_{Un}\cdot \ldots \cdot H^H_{U2}\cdot H^H_{U1}\cdot I\\
\rightarrow\;\;\;\; U_B\; &= I\cdot H_{Un}\cdot \ldots \cdot H_{U2}\cdot H_{U1}
\end{align}

Recall that the leading portion of each Householder matrix is the
identity except the first.  Thus, at the beginning of backward
accumulation, $U_B$ is ``mostly the identity'' and it gradually
becomes full as the iteration progresses.  This pattern can be
exploited to reduce the number of required flops.  In contrast,
$U_B^H$ is full in forward accumulation after the first step.  For
this reason, backward accumulation is cheaper and the strategy of
choice.  When unpacking the $U_B$ matrix in place of the original $A$
matrix it is necessary to choose backward accumulation anyway.
\begin{align}
V_B\; &= I\cdot H^H_{V1}\cdot H^H_{V2}\cdot \ldots \cdot H^H_{Vn}\\
\rightarrow\;\;\;\; V_B^H &= I\cdot H_{Vn}\cdot \ldots \cdot H_{V2}\cdot H_{V1}
\end{align}

Unpacking the $V_B^H$ matrix is done in a similar way also performing
successive Housholder matrix multiplications using backward
accumulation.

\subsubsection{Step 3: Diagonalization -- shifted QR iteration}

At this stage the matrices $U_B$ and $V_B^H$ exist in unfactored form.
Also there are the diagonal vector $\beta$ and the super-diagonal
vector $\delta$.
\begin{equation}
B^{(0)} = 
\begin{bmatrix}
\beta_1 & \delta_2 & 0 & \cdots & 0\\
0 & \beta_2 & \delta_3 & 0 & 0\\
\vdots & 0 & \ddots & \ddots & 0\\
0 & 0 & 0 & \beta_{n-1} & \delta_n\\
0 & 0 & 0 & 0 & \beta_n\\
\end{bmatrix}
\end{equation}

Computation of the eigenvalue $\mu$ of the 2-by-2 submatrix of $T_n =
B_n^T\cdot B_n$ that is closer to the $t_{22}$ matrix element.
\begin{align}
T_n =
\begin{bmatrix}
t_{11} & t_{12}\\
t_{21} & t_{22}\\
\end{bmatrix}
= B_n^T\cdot B_n &=
\begin{bmatrix}
\beta_{n-1} & 0\\
\delta_{n} & \beta_{n}\\
\end{bmatrix} \cdot
\begin{bmatrix}
\beta_{n-1} & \delta_{n}\\
0 & \beta_{n}\\
\end{bmatrix}\\
&=
\begin{bmatrix}
\beta_{n-1}^2 & \delta_{n}\cdot \beta_{n-1}\\
\delta_{n}\cdot \beta_{n-1} & \beta_{n}^2 + \delta_{n}^2\\
\end{bmatrix}
\end{align}

Wilkinson shift.
\begin{align}
\mu &= t_{22} + d - sign(d)\cdot \sqrt{d^2 + t_{12}^2}\\
&= t_{22} + d - t_{12}^2\cdot sign\left(\dfrac{d}{t_{12}}\right)\cdot \sqrt{\left(\dfrac{d}{t_{12}}\right)^2 + 1}
\end{align}
whereas
\begin{equation}
d = \dfrac{t_{11} - t_{22}}{2}
\end{equation}

\subsubsection{Step 4: Solving the equation system}

It is straight-forward to solve a given equation system once having
the singular value decomposition computed.
\begin{align}
A\cdot x &= z\\
U\Sigma V^H\cdot x &= z\\
\Sigma V^H\cdot x &= U^H \cdot z\\
V^H\cdot x &= \Sigma^{-1} U^H \cdot z\\
x &= V\Sigma^{-1} U^H \cdot z
\label{eq:svd_solution}
\end{align}

The inverse of the diagonal matrix $\Sigma$ yields
\begin{equation}
\Sigma^{-1} =
\begin{bmatrix}
1/\sigma_1 & 0 & \cdots & 0\\
0 & 1/\sigma_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1/\sigma_n
\end{bmatrix}
\end{equation}

With $v_i$ being the i-th row of the matrix $V$, $u_i$ the i-th column
of the matrix $U$ and $\sigma_i$ the i-th singular value
eq.~\eqref{eq:svd_solution} can be rewritten to
\begin{equation}
x = \sum_{i=1}^{n} \dfrac{u_i^H\cdot z}{\sigma_i}\cdot v_i
\end{equation}

It must be mentioned that very small singular values $\sigma_i$
corrupt the complete result.  Such values indicate (nearly) singular
(ill-conditioned) matrices $A$.  In such cases, the solution vector
$x$ obtained by zeroing the small $\sigma_i$'s and then using equation
\eqref{eq:svd_solution} is better than direct-method solutions (such
as LU decomposition or Gaussian elimination) and the SVD solution
where the small $\sigma_i$'s are left non-zero. It may seem
paradoxical that this can be so, since zeroing a singular value
corresponds to throwing away one linear combination of the set of
equations that is going to be solved.  The resolution of the paradox
is that a combination of equations that is so corrupted by roundoff
error is thrown away precisely as to be at best useless; usually it is
worse than useless since it "pulls" the solution vector way off
towards infinity along some direction that is almost a nullspace
vector.

\subsection{Jacobi method}
%\addcontentsline{toc}{subsection}{Jacobi method}

This method quite simply involves rearranging each equation to make
each variable a function of the other variables.  Then make an initial
guess for each solution and iterate.  For this method it is necessary
to ensure that all the diagonal matrix elements $a_{ii}$ are non-zero.
This is given for the nodal analysis and almostly given for the
modified nodal analysis.  If the linear equation system is solvable
this can always be achieved by rows substitutions.

\addvspace{12pt}

The algorithm for performing the iteration step $k + 1$ writes as
follows.
\begin{equation}
x_{i}^{(k+1)} = \dfrac{1}{a_{ii}}\left(z_i - \sum_{j=1}^{i-1} a_{ij}x_{j}^{(k)} - \sum_{j=i+1}^{n} a_{ij}x_{j}^{(k)}\right)
\;\;\;\; \textrm{ for } i = 1, \ldots, n
\end{equation}

This has to repeated until the new solution vectors $x^{(k+1)}$
deviation from the previous one $x^{(k)}$ is sufficiently small.

\addvspace{12pt}

The initial guess has no effect on whether the iterative method
converges or not, but with a good initial guess (as possibly given in
consecutive Newton-Raphson iterations) it converges faster (if it
converges).  To ensure convergence the condition
\begin{equation}
\sum_{j = 1, j \ne i}^{n} \left|a_{ij}\right| \le \left|a_{ii}\right|
\;\;\;\; \textrm{ for } i = 1, \ldots, n
\end{equation}

and at least one case
\begin{equation}
\sum_{i = 1, i \ne j}^{n} \left|a_{ij}\right| \le \left|a_{ii}\right|
\end{equation}

must apply.  If these conditions are not met, the iterative equations
may still converge.  If these conditions are met the iterative
equations will definitely converge.

\addvspace{12pt}

Another simple approach to a convergence criteria for iterative
algorithms is the Schmidt and v. Mises criteria.
\begin{equation}
\sqrt{\sum_{i = 1}^n \sum_{j = 1, j \ne i}^n \left|\dfrac{a_{ij}}{a_{ii}}\right|^2} < 1
\end{equation}

\subsection{Gauss-Seidel method}
%\addcontentsline{toc}{subsection}{Gauss-Seidel method}

The Gauss-Seidel algorithm is a modification of the Jacobi method.  It
uses the previously computed values in the solution vector of the same
iteration step.  That is why this iterative method is expected to
converge faster than the Jacobi method.

\addvspace{12pt}

The slightly modified algorithm for performing the $k + 1$ iteration
step writes as follows.
\begin{equation}
x_{i}^{(k+1)} = \dfrac{1}{a_{ii}}\left(z_i - \sum_{j=1}^{i-1} a_{ij}x_{j}^{(k+1)} - \sum_{j=i+1}^{n} a_{ij}x_{j}^{(k)}\right)
\;\;\;\; \textrm{ for } i = 1, \ldots, n
\end{equation}

The remarks about the initial guess $x^{(0)}$ as well as the
convergence criteria noted in the section about the Jacobi method
apply to the Gauss-Seidel algorithm as well.

\subsection{A comparison}
%\addcontentsline{toc}{subsection}{A comparison}

There are direct and iterative methods (algorithms) for solving linear
equation systems.  Equation systems with large and sparse matrices
should rather be solved with iterative methods.

\addvspace{12pt}

\begin{tabular}{|p{2.2cm}|p{1.5cm}|p{1.8cm}|p{2.1cm}|p{1.7cm}|p{2.95cm}|}
\hline
\raggedright method & \raggedleft precision & \raggedleft application & 
\raggedleft programming effort & \raggedleft computing complexity & 
\parbox[t]{2.95cm}{\raggedleft notes}\\
\hline
\raggedright Laplace expansion & \raggedleft numerical errors & 
\raggedleft general & \raggedleft straight forward &
\raggedleft $n!$ & \parbox[t]{2.95cm}{\raggedleft very time consuming}\\
\hline
\raggedright Gaussian elimination & \raggedleft numerical errors & 
\raggedleft general & \raggedleft intermediate & \raggedleft $n^3/3 + n^2/2$ &
\parbox[t]{2.95cm}{\raggedleft }\\
\hline
\raggedright Gauss-Jordan & \raggedleft numerical errors & \raggedleft general & \raggedleft intermediate & \raggedleft $n^3/3 + n^2 - n/3$ & \parbox[t]{2.95cm}{\raggedleft computes the inverse besides}\\
\hline
\raggedright LU decomposition & \raggedleft numerical errors & \raggedleft general & \raggedleft intermediate & \raggedleft $n^3/3 + n^2 - n/3$ & \parbox[t]{2.95cm}{\raggedleft useful for consecutive solutions}

\addvspace{1pt}

\\
\hline
\raggedright QR decomposition & \raggedleft good & \raggedleft general & \raggedleft high & \raggedleft $2n^3 + 3n^3$ & \parbox[t]{2.95cm}{\raggedleft }\\
\hline
\raggedright Singular value decomposition & \raggedleft good & \raggedleft general & \raggedleft very high & \raggedleft $2n^3 + 4n^3$ & \parbox[t]{2.95cm}{\raggedleft ill-conditioned matrices can be handled}

\addvspace{1pt}

\\
\hline
\raggedright Jacobi & \raggedleft very good & \raggedleft diagonally dominant systems & \raggedleft easy & \raggedleft $n^2$ in each iteration step & \parbox[t]{2.95cm}{\raggedleft possibly no convergence}\\
\hline
\raggedright Gauss-Seidel & \raggedleft very good & \raggedleft diagonally dominant systems & \raggedleft easy & \raggedleft $n^2$ in each iteration step & \parbox[t]{2.95cm}{\raggedleft possibly no convergence}\\
\hline
\end{tabular}

\section{Polynomial approximations}
%\addcontentsline{toc}{section}{Polynomial approximations}

\subsection{Cubic splines}
%\addcontentsline{toc}{subsection}{Cubic splines}
